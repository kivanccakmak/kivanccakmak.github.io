\documentclass[11pt]{article}
\usepackage{tikz}
\usetikzlibrary{trees}
\usepackage{forest}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{graphicx}


%Gummi|065|=)
\title{\textbf{Shannon Entropy}}
\author{K{\i}van\c{c} \c{C}akmak\\}
\date{}
\begin{document}

\maketitle

In this article, I'll try to explain Shannon Entropy with one of it's use case: data-compression. First, I'll provide a small introduction about data-representation and a need for compression with a very brief information about it's requirements. Consequently, I'll derive the proof of well known entropy formula in the equation below:

\begin{equation}
H(X) = -\sum_{i}p_{i}\log_{b}p_{i}
\label{Eq:entropy}
\end{equation}

\section{Data Representation and Compression}

We all know that the way that our digital devices store data is ones and zeros, aka in binary format. So, our understanding of the smallest chunk of information is: "whether there is a voltage or not". 
We represent letters of a text, pixels of an image, etc. in binary format. 

Below, I provide an ASCII (American Standard Code for Information Interchange) encoded \textit{hello} in Table \ref{table:hello}. In ASCII, each letter -and other control characters, such as \texttt{DEL} and numbers like $1$, $2$ etc.- consists $8$ bits. So, the text that we see is \texttt{hello}, but the information that a digital device store is: 
\begin{center}
    \texttt{0110100001100101011011000110110001101111}
\end{center}

\begin{figure}
\begin{tabular}{| l | l | l | l | l | l |}
 \hline
  text   & h & e & l & l & o\\ \hline
  ASCII  & 104 & 101 & 108 & 108 & 111 \\ \hline 
  binary & 01101000 & 01100101 & 01101100 & 01101100 & 01101111 \\ 
  \hline
\end{tabular}
\caption{ASCII encoding of \textit{hello}.}
\label{table:hello}
\end{figure}

So, there are $26$ letters in the English alphabet, and we can say that $5$ bits would be enough to encode all characters, since $2^{5} = 32 > 26$. Nevertheless, ASCII has 127 different values --See Reference \cite{ascii}, which at least requires 7 bits per value. 

\newpage
The rest of the document tries to answer the following question:

\begin{center}
\textit{What would be the minimum number of bits that we have to use, if we knew the distribution of letters in the text?}
\end{center}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{img/english_histogram.png}
\caption{Frequency Table of English Letters \label{fig:letter_histogram}}
\end{figure}

So, as it could be seen in the Figure \ref{fig:letter_histogram} --and we all very well know, some of the letters are more oftenly used than the others. Here, \textit{e} is the most commonly used letter in English. So, would it be better to represent \textit{e} with less bits than the others, if the letter distribution of our text would be exactly same with Figure \ref{fig:letter_histogram}. If so, how less could it be?

\section{Uniquely Decodable Codes}
A code is a mapping of \textit{source messages} into \textit{codewords}. As an exampe, ASCII code above maps letters (as a source message) to codewords --such as \textit{h} to 01101000. 

Here, a code is \textit{distinct} if each codeword is distinguishable from every other (i.e., the mapping from source messages to codewords is one-to-one). A distinct code is \textit{uniquely decodable} if every codeword is identifiable when immersed in a sequence of codewords. 

\subsection{Prefix Codes}
A uniquely decodable code is a \textit{prefix code} (or prefix-free code) if it has the prefix property, which requires that no codeword is a proper prefix of any other codeword.

\begin{figure}[h!]
\begin{center}
\begin{tabular}{| l | l | l | l | l |}
 \hline
  text   & a & b & c & d \\ \hline
  binary & 0 & 01 & 011 & 0111 \\ 
  \hline
\end{tabular}
\caption{Uniquely decodable non-prefix code}
\label{table:non_prefix_code}
\end{center}
\end{figure}

The code in Table \ref{table:non_prefix_code} is decodable but it is not a prefix code because the codeword for \textit{a} is a prefix for the codeword for \textit{b}. This means that we can not instantaneously decode \textit{a} without waiting for the next bit of data (to determine whether it is actually \textit{a} or just the first half of \textit{b}.)

\begin{figure}[h!]
\begin{center}
\begin{tabular}{| l | l | l | l | l |}
 \hline
  text   & a & b & c & d \\ \hline
  binary & 0 & 01 & 011 & 111 \\ 
  \hline
\end{tabular}
\caption{Uniquely decodable non-prefix code}
\label{table:prefix_code}
\end{center}
\end{figure}

Alternatively, the code in Table \ref{table:prefix_code} is a prefix code, since no codeword is a prefix of  another codeword.

\section{Kraft's Inequality}

Kraft's inequality states that given a list of positive integers ($n_{1}, n_{2}, \dots, n_{r}$), there exists a prefix code with a set of codewords ($\sigma_{1}, \sigma_{2}, \dots, \sigma_{r}$) where the length of each codeword $|\sigma_{i}| = n_{i}$, $\forall_{i}$, if and only if:

\begin{equation}
\sum_{i=1}^{r} s^{-n_{i}} \leq 1
\end{equation}

where \textit{s} is the size of the alphabet \textit{S}.

Our previous example code in Table \ref{table:prefix_code}, the alphabet is simply the binary digits $S = {0, 1}$, and therefore the size of the alphabet $s = 2$. We can easily check that (see Figure \ref{fig:kraft_tree}) indeed the inequality holds:

\begin{equation}
2^{-1} + 2^{-2} + 2^{-3} + 2^{-3} \leq 1
\end{equation}

\begin{center}
\begin{figure}[h!]
\begin{center}
\begin{forest}
for tree={circle,draw, l sep=1cm, s sep=0.6cm, minimum height=1cm, minimum width=1cm}
[, 
    [1,edge label={node[midway,left] {$2^{-1}$}}
        [11,edge label={node[midway,left] {$2^{-2}$}}
            [111,edge label={node[midway,left] {$2^{-3}$}}]
            [110,edge label={node[midway,right] {$2^{-3}$}}]
        ] 
        [10,edge label={node[midway,right] {$2^{-2}$}}
            [101,edge label={node[midway,left] {$2^{-3}$}}]
            [100,edge label={node[midway,right] {$2^{-3}$}}]
        ] 
    ]
    [0,edge label={node[midway,right] {$2^{-1}$}}
        [01,edge label={node[midway,left] {$2^{-2}$}}
            [011,edge label={node[midway,left] {$2^{-3}$}}]
            [010,edge label={node[midway,right] {$2^{-3}$}}]
        ] 
        [00,edge label={node[midway,right] {$2^{-2}$}}
            [001,edge label={node[midway,left] {$2^{-3}$}}]
            [000,edge label={node[midway,right] {$2^{-3}$}}]
        ] 
    ] 
]
\end{forest}
\end{center}
\caption{Code Tree}
\label{fig:kraft_tree}
\end{figure}
\end{center}


\begin{thebibliography}{99}


\bibitem{ascii}
\url{http://ee.hawaii.edu/~tep/EE160/Book/chap4/subsection2.1.1.1.html}

\bibitem{mortada_krafts}
\url{https://mortada.net/simple-proof-for-krafts-inequality.html}

\bibitem{fundemental_concepts}
\url{https://www.ics.uci.edu/~dan/pubs/DC-Sec1.html}

\end{thebibliography}


\end{document}
